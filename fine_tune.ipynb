{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install scikit-learn\n",
    "# %pip install datasets\n",
    "# %pip install accelerate -U\n",
    "# %pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments, default_data_collator\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/lines/3-70-0.jpeg</td>\n",
       "      <td>11/12/23 the night Pearl died. Notes from the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/lines/2-37-16.jpeg</td>\n",
       "      <td>12:34am and eventually they'll come to you. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/lines/1-20-17.jpeg</td>\n",
       "      <td>lease for optionality + look of any better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/lines/2-61-2.jpeg</td>\n",
       "      <td>cont. you loved, and love sex is way better. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/lines/2-63-13.jpeg</td>\n",
       "      <td>I was trying to get at in my note abt \"you are</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file_name                                               text\n",
       "0   data/lines/3-70-0.jpeg  11/12/23 the night Pearl died. Notes from the ...\n",
       "1  data/lines/2-37-16.jpeg  12:34am and eventually they'll come to you. Go...\n",
       "2  data/lines/1-20-17.jpeg         lease for optionality + look of any better\n",
       "3   data/lines/2-61-2.jpeg  cont. you loved, and love sex is way better. I...\n",
       "4  data/lines/2-63-13.jpeg     I was trying to get at in my note abt \"you are"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_fwf('fine_tune_examples.txt', header = None, encoding = 'ISO-8859-1')\n",
    "df['file_name'] = [df.iloc[i, 0].split(\" \")[0] for i in range(len(df))]\n",
    "df['text'] = [\" \".join(df[0].iloc[i].split(\" \")[1:]) for i in range(len(df))]\n",
    "df = df.drop(0, axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "# we reset the indices to start from zero\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "train_dataset = IAMDataset(root_dir='',\n",
    "                           df=train_df,\n",
    "                           processor=processor)\n",
    "eval_dataset = IAMDataset(root_dir='',\n",
    "                           df=test_df,\n",
    "                           processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pithy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    fp16=False, \n",
    "    output_dir=\"model/\",\n",
    "    logging_steps=1,\n",
    "    save_steps=1000,\n",
    "    eval_steps=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTrainingArguments(output_dir='model/', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, lr_scheduler_kwargs={}, warmup_ratio=0.0, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='model/runs\\\\Feb23_18-22-50_DESKTOP-VTBMBCU', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=1, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=1000, save_total_limit=None, save_safetensors=True, save_on_each_node=False, save_only_model=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=200, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name='model/', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True), deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, sortish_sampler=False, predict_with_generate=True, generation_max_length=None, generation_num_beams=None, generation_config=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-95fcaadde678>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric(\"cer\")\n",
      "C:\\Users\\pithy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\datasets\\load.py:753: FutureWarning: The repository for cer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/cer/cer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cer_metric = load_metric(\"cer\")\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pithy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\transformers\\models\\trocr\\processing_trocr.py:136: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2b38c5fa0648859760a20fd83c2760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.2314, 'grad_norm': 62.57602310180664, 'learning_rate': 4.930555555555556e-05, 'epoch': 0.04}\n",
      "{'loss': 8.4539, 'grad_norm': 38.18719482421875, 'learning_rate': 4.8611111111111115e-05, 'epoch': 0.08}\n",
      "{'loss': 7.333, 'grad_norm': 34.19308853149414, 'learning_rate': 4.791666666666667e-05, 'epoch': 0.12}\n",
      "{'loss': 7.2197, 'grad_norm': 24.368227005004883, 'learning_rate': 4.722222222222222e-05, 'epoch': 0.17}\n",
      "{'loss': 6.5196, 'grad_norm': 17.323875427246094, 'learning_rate': 4.652777777777778e-05, 'epoch': 0.21}\n",
      "{'loss': 6.5083, 'grad_norm': 21.561010360717773, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}\n",
      "{'loss': 6.2791, 'grad_norm': 16.505128860473633, 'learning_rate': 4.5138888888888894e-05, 'epoch': 0.29}\n",
      "{'loss': 6.0409, 'grad_norm': 22.271398544311523, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}\n",
      "{'loss': 6.6672, 'grad_norm': 23.66922950744629, 'learning_rate': 4.375e-05, 'epoch': 0.38}\n",
      "{'loss': 6.7896, 'grad_norm': 17.559350967407227, 'learning_rate': 4.305555555555556e-05, 'epoch': 0.42}\n",
      "{'loss': 5.9569, 'grad_norm': 13.951250076293945, 'learning_rate': 4.236111111111111e-05, 'epoch': 0.46}\n",
      "{'loss': 5.9988, 'grad_norm': 13.672079086303711, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n",
      "{'loss': 6.4325, 'grad_norm': 13.755288124084473, 'learning_rate': 4.0972222222222225e-05, 'epoch': 0.54}\n",
      "{'loss': 6.0429, 'grad_norm': 14.211644172668457, 'learning_rate': 4.027777777777778e-05, 'epoch': 0.58}\n",
      "{'loss': 6.2548, 'grad_norm': 12.779500007629395, 'learning_rate': 3.958333333333333e-05, 'epoch': 0.62}\n",
      "{'loss': 6.5514, 'grad_norm': 14.304454803466797, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}\n",
      "{'loss': 5.7735, 'grad_norm': 12.984872817993164, 'learning_rate': 3.8194444444444444e-05, 'epoch': 0.71}\n",
      "{'loss': 6.4744, 'grad_norm': 12.745645523071289, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}\n",
      "{'loss': 6.058, 'grad_norm': 13.541937828063965, 'learning_rate': 3.6805555555555556e-05, 'epoch': 0.79}\n",
      "{'loss': 5.7974, 'grad_norm': 19.817909240722656, 'learning_rate': 3.611111111111111e-05, 'epoch': 0.83}\n",
      "{'loss': 5.6396, 'grad_norm': 22.014328002929688, 'learning_rate': 3.541666666666667e-05, 'epoch': 0.88}\n",
      "{'loss': 5.1708, 'grad_norm': 21.89234161376953, 'learning_rate': 3.472222222222222e-05, 'epoch': 0.92}\n",
      "{'loss': 4.7913, 'grad_norm': 30.52900505065918, 'learning_rate': 3.402777777777778e-05, 'epoch': 0.96}\n",
      "{'loss': 4.3371, 'grad_norm': 29.363096237182617, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
      "{'loss': 3.6353, 'grad_norm': 33.50663757324219, 'learning_rate': 3.263888888888889e-05, 'epoch': 1.04}\n",
      "{'loss': 3.4523, 'grad_norm': 63.482200622558594, 'learning_rate': 3.194444444444444e-05, 'epoch': 1.08}\n",
      "{'loss': 2.6945, 'grad_norm': 29.06781005859375, 'learning_rate': 3.125e-05, 'epoch': 1.12}\n",
      "{'loss': 2.5283, 'grad_norm': 45.1621208190918, 'learning_rate': 3.055555555555556e-05, 'epoch': 1.17}\n",
      "{'loss': 2.0575, 'grad_norm': 90.77035522460938, 'learning_rate': 2.9861111111111113e-05, 'epoch': 1.21}\n",
      "{'loss': 2.474, 'grad_norm': 82.91596221923828, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n",
      "{'loss': 1.9874, 'grad_norm': 39.62946701049805, 'learning_rate': 2.8472222222222223e-05, 'epoch': 1.29}\n",
      "{'loss': 1.9983, 'grad_norm': 45.91099166870117, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}\n",
      "{'loss': 1.8202, 'grad_norm': 36.972957611083984, 'learning_rate': 2.7083333333333332e-05, 'epoch': 1.38}\n",
      "{'loss': 1.4481, 'grad_norm': 60.951194763183594, 'learning_rate': 2.6388888888888892e-05, 'epoch': 1.42}\n",
      "{'loss': 1.5537, 'grad_norm': 129.7755584716797, 'learning_rate': 2.5694444444444445e-05, 'epoch': 1.46}\n",
      "{'loss': 1.6709, 'grad_norm': 26.011865615844727, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n",
      "{'loss': 2.2893, 'grad_norm': 103.98141479492188, 'learning_rate': 2.4305555555555558e-05, 'epoch': 1.54}\n",
      "{'loss': 2.5656, 'grad_norm': 103.5674819946289, 'learning_rate': 2.361111111111111e-05, 'epoch': 1.58}\n",
      "{'loss': 2.361, 'grad_norm': 68.627197265625, 'learning_rate': 2.2916666666666667e-05, 'epoch': 1.62}\n",
      "{'loss': 1.6194, 'grad_norm': 54.522560119628906, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}\n",
      "{'loss': 1.7665, 'grad_norm': 72.04548645019531, 'learning_rate': 2.152777777777778e-05, 'epoch': 1.71}\n",
      "{'loss': 1.8021, 'grad_norm': 33.379493713378906, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}\n",
      "{'loss': 1.6458, 'grad_norm': 66.91127014160156, 'learning_rate': 2.013888888888889e-05, 'epoch': 1.79}\n",
      "{'loss': 1.8877, 'grad_norm': 36.18568420410156, 'learning_rate': 1.9444444444444445e-05, 'epoch': 1.83}\n",
      "{'loss': 1.567, 'grad_norm': 48.79549789428711, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.88}\n",
      "{'loss': 2.3851, 'grad_norm': 121.72086334228516, 'learning_rate': 1.8055555555555555e-05, 'epoch': 1.92}\n",
      "{'loss': 1.5959, 'grad_norm': 41.88117218017578, 'learning_rate': 1.736111111111111e-05, 'epoch': 1.96}\n",
      "{'loss': 1.5101, 'grad_norm': 31.089017868041992, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
      "{'loss': 1.4934, 'grad_norm': 45.553401947021484, 'learning_rate': 1.597222222222222e-05, 'epoch': 2.04}\n",
      "{'loss': 1.3963, 'grad_norm': 48.44227981567383, 'learning_rate': 1.527777777777778e-05, 'epoch': 2.08}\n",
      "{'loss': 1.083, 'grad_norm': 34.3011589050293, 'learning_rate': 1.4583333333333335e-05, 'epoch': 2.12}\n",
      "{'loss': 1.3257, 'grad_norm': 66.85179901123047, 'learning_rate': 1.388888888888889e-05, 'epoch': 2.17}\n",
      "{'loss': 1.1226, 'grad_norm': 81.52554321289062, 'learning_rate': 1.3194444444444446e-05, 'epoch': 2.21}\n",
      "{'loss': 0.8201, 'grad_norm': 27.09469223022461, 'learning_rate': 1.25e-05, 'epoch': 2.25}\n",
      "{'loss': 0.8983, 'grad_norm': 20.94473648071289, 'learning_rate': 1.1805555555555555e-05, 'epoch': 2.29}\n",
      "{'loss': 0.6781, 'grad_norm': 15.662236213684082, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}\n",
      "{'loss': 0.7654, 'grad_norm': 37.19968795776367, 'learning_rate': 1.0416666666666668e-05, 'epoch': 2.38}\n",
      "{'loss': 0.8445, 'grad_norm': 71.78074645996094, 'learning_rate': 9.722222222222223e-06, 'epoch': 2.42}\n",
      "{'loss': 0.5706, 'grad_norm': 20.69208526611328, 'learning_rate': 9.027777777777777e-06, 'epoch': 2.46}\n",
      "{'loss': 0.7175, 'grad_norm': 22.293821334838867, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n",
      "{'loss': 0.9138, 'grad_norm': 41.03291320800781, 'learning_rate': 7.63888888888889e-06, 'epoch': 2.54}\n",
      "{'loss': 1.0874, 'grad_norm': 262.6247253417969, 'learning_rate': 6.944444444444445e-06, 'epoch': 2.58}\n",
      "{'loss': 0.8001, 'grad_norm': 25.042030334472656, 'learning_rate': 6.25e-06, 'epoch': 2.62}\n",
      "{'loss': 0.6189, 'grad_norm': 20.61319351196289, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}\n",
      "{'loss': 0.6688, 'grad_norm': 14.919368743896484, 'learning_rate': 4.861111111111111e-06, 'epoch': 2.71}\n",
      "{'loss': 0.7, 'grad_norm': 15.456869125366211, 'learning_rate': 4.166666666666667e-06, 'epoch': 2.75}\n",
      "{'loss': 1.8693, 'grad_norm': 24.480270385742188, 'learning_rate': 3.4722222222222224e-06, 'epoch': 2.79}\n",
      "{'loss': 0.6696, 'grad_norm': 21.664714813232422, 'learning_rate': 2.777777777777778e-06, 'epoch': 2.83}\n",
      "{'loss': 1.2316, 'grad_norm': 70.75532531738281, 'learning_rate': 2.0833333333333334e-06, 'epoch': 2.88}\n",
      "{'loss': 0.5711, 'grad_norm': 16.53411293029785, 'learning_rate': 1.388888888888889e-06, 'epoch': 2.92}\n",
      "{'loss': 0.634, 'grad_norm': 23.85772132873535, 'learning_rate': 6.944444444444445e-07, 'epoch': 2.96}\n",
      "{'loss': 0.6717, 'grad_norm': 20.09658432006836, 'learning_rate': 0.0, 'epoch': 3.0}\n",
      "{'train_runtime': 8544.5616, 'train_samples_per_second': 0.067, 'train_steps_per_second': 0.008, 'train_loss': 3.1220834760202303, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=72, training_loss=3.1220834760202303, metrics={'train_runtime': 8544.5616, 'train_samples_per_second': 0.067, 'train_steps_per_second': 0.008, 'train_loss': 3.1220834760202303, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_processor = []\n",
    "for filename in os.listdir(\"data/lines/\")[15:25]:\n",
    "    image = Image.open(f\"data/lines/{filename.lower()}\").convert(\"RGB\")\n",
    "\n",
    "    processor = processor\n",
    "    model = model\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    model_processor.append(f\"{filename} {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1-100-22.jpeg something that shed think about it more, more',\n",
       " '1-100-23.jpeg her opinion, and disom me. In many ways this',\n",
       " '1-100-24.jpeg incident was one of my worst team.',\n",
       " '1-100-25.jpeg a creep w/ no control over the situation.',\n",
       " '1-100-26.jpeg The concert was okay. More tracking! And you',\n",
       " '1-100-27.jpeg anything backup new rep. rep. How rep w/ were less challenging',\n",
       " '1-100-28.jpeg orghtta balance new rep w/ meee less challenging.',\n",
       " '1-100-29.jpeg styles. There were some beautiful women in',\n",
       " '1-100-3.jpeg amudant, but for the 12-18 months it seems',\n",
       " '1-100-30.jpeg there. It made me sold where Also someone']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "pt_proc = []\n",
    "for filename in os.listdir(\"data/lines/\")[0:25]:\n",
    "    image = Image.open(f\"data/lines/{filename.lower()}\").convert(\"RGB\")\n",
    "\n",
    "    processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-stage1\")\n",
    "    model = model\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    pt_proc.append(f\"{filename} {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"1-100-0.jpeg 4/23/23 her take the lead on it and she hasn't record\",\n",
       " '1-100-1.jpeg cont. of to me. Driving out to UNCG tin to see',\n",
       " '1-100-10.jpeg writing that! He and I sat and talked and',\n",
       " '1-100-11.jpeg In the living in the living room, about NC Publishers and',\n",
       " '1-100-12.jpeg jabs and grad school school and dating and operational,',\n",
       " '1-100-13.jpeg eventually by sam, until Riley tested me to get',\n",
       " '1-100-14.jpeg eventually by som, until Riley tested me to get',\n",
       " '1-100-15.jpeg dinner. I picked her up from Lanzas, where every',\n",
       " '1-100-16.jpeg person my age in Carrboro has been hiding all',\n",
       " '1-100-17.jpeg this time, apparently, and we got food at Weaver.',\n",
       " '1-100-18.jpeg I told her about last Saturday and she brushed',\n",
       " '1-100-19.jpeg It at like it was nothing, barely working',\n",
       " '1-100-2.jpeg Jada Patent says & conduct. I don\\'t \"',\n",
       " '1-100-20.jpeg about. So that made me feel a little better.',\n",
       " '1-100-21.jpeg Then I spent the whole drive to Greensboro',\n",
       " \"1-100-22.jpeg something that she'd think about it more, more\",\n",
       " '1-100-23.jpeg her option, and disam me. In many ways this',\n",
       " '1-100-24.jpeg incident was one of my worst team. Became',\n",
       " '1-100-25.jpeg a creep w/ no control over the situation.',\n",
       " '1-100-26.jpeg The concert was okay. Maybe tracking! And you you',\n",
       " '1-100-27.jpeg ought to balance new rep w/ we were less challenging',\n",
       " '1-100-28.jpeg orghtra backup new rep w/ meee less challenging',\n",
       " '1-100-29.jpeg styles. There were some beautiful women in',\n",
       " \"1-100-3.jpeg aw don't, but for the 12-18 months it seems\",\n",
       " '1-100-30.jpeg there. It made me sold, where Also someone']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: 'VisionEncoderDecoderModel(\n  (encoder): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViTPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): TrOCRForCausalLM(\n    (model): TrOCRDecoderWrapper(\n      (decoder): TrOCRDecoder(\n        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n        (embed_positions): TrOCRSinusoidalPositionalEmbedding()\n        (layers): ModuleList(\n          (0-11): 12 x TrOCRDecoderLayer(\n            (self_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n  )\n)'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[0;32m    399\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"repo_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"from_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"to_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mREPO_ID_REGEX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         raise HFValidationError(\n\u001b[0m\u001b[0;32m    165\u001b[0m             \u001b[1;34m\"Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'VisionEncoderDecoderModel(\n  (encoder): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViTPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): TrOCRForCausalLM(\n    (model): TrOCRDecoderWrapper(\n      (decoder): TrOCRDecoder(\n        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n        (embed_positions): TrOCRSinusoidalPositionalEmbedding()\n        (layers): ModuleList(\n          (0-11): 12 x TrOCRDecoderLayer(\n            (self_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n  )\n)'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-36777baf80f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"data/lines/{filename.lower()}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RGB\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrOCRProcessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpixel_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\transformers\\processing_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m    463\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"token\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_arguments_from_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m         \u001b[0mprocessor_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_processor_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\transformers\\processing_utils.py\u001b[0m in \u001b[0;36m_get_arguments_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m                 \u001b[0mattribute_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformers_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m             \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattribute_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\transformers\\models\\auto\\image_processing_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_from_auto\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageProcessingMixin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_image_processor_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m         \u001b[0mimage_processor_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"image_processor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[0mimage_processor_auto_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\transformers\\image_processing_utils.py\u001b[0m in \u001b[0;36mget_image_processor_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m                 \u001b[1;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m                 resolved_image_processor_file = cached_file(\n\u001b[0m\u001b[0;32m    335\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m                     \u001b[0mimage_processor_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{err}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mHFValidationError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m         raise EnvironmentError(\n\u001b[0m\u001b[0;32m    463\u001b[0m             \u001b[1;34mf\"Incorrect path_or_model_id: '{path_or_repo_id}'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m         ) from e\n",
      "\u001b[1;31mOSError\u001b[0m: Incorrect path_or_model_id: 'VisionEncoderDecoderModel(\n  (encoder): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViTPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): TrOCRForCausalLM(\n    (model): TrOCRDecoderWrapper(\n      (decoder): TrOCRDecoder(\n        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n        (embed_positions): TrOCRSinusoidalPositionalEmbedding()\n        (layers): ModuleList(\n          (0-11): 12 x TrOCRDecoderLayer(\n            (self_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n  )\n)'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "from_pretrained_model = []\n",
    "for filename in os.listdir(\"data/lines/\")[0:10]:\n",
    "    image = Image.open(f\"data/lines/{filename.lower()}\").convert(\"RGB\")\n",
    "\n",
    "    processor = TrOCRProcessor.from_pretrained(model)\n",
    "    model = model\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    pt_proc.append(f\"{filename} {generated_text}\")\n",
    "from_pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"1-100-0.jpeg 4/23/23 here the lead on it and she haven't read\",\n",
       " '1-100-1.jpeg cont. out to me. Driving out to Uncle tn to see',\n",
       " '1-100-10.jpeg writing that! He and I sat and talked and',\n",
       " \"1-100-11.jpeg longed in the living room, about NC Polb's and\",\n",
       " '1-100-12.jpeg jobs and sad school and dating and opera, joined',\n",
       " '1-100-13.jpeg eventually by som, until Riley testified me to get',\n",
       " '1-100-14.jpeg eventually by som, until Riley testified me to get',\n",
       " '1-100-15.jpeg dinear. I picked her up from Lawyers, where every',\n",
       " '1-100-16.jpeg person my age in Carrboro has been hiding all',\n",
       " '1-100-17.jpeg this time, apparently, and we got food at Weaver.',\n",
       " '1-100-18.jpeg I told her about last Saturday and she brushed',\n",
       " '1-100-19.jpeg let like it was nothing, barely worth talking',\n",
       " '1-100-2.jpeg 7ada potent song & conduct. I don\\'t \"deal\"',\n",
       " '1-100-20.jpeg about. So that. make we feel a little better.',\n",
       " '1-100-21.jpeg Then I spent the whole drive to Greensboro',\n",
       " \"1-100-22.jpeg contiking that she'd think about it more, make\",\n",
       " '1-100-23.jpeg her opinion, and dinner me. In was this this this',\n",
       " '1-100-24.jpeg incident was one of my worst fear. Because',\n",
       " '1-100-25.jpeg a crop w/ no control over the situation.',\n",
       " '1-100-26.jpeg The concert was okay. More fucking! And you',\n",
       " '1-100-27.jpeg owght.a balance new rep where loss challenge',\n",
       " '1-100-28.jpeg orghtn balance new rep w/ were less challenging',\n",
       " '1-100-29.jpeg styles. There were some beautiful women in',\n",
       " '1-100-3.jpeg avoident, but for the 18-18 months it seems',\n",
       " '1-100-30.jpeg there. It made we so so more Also someone',\n",
       " \"1-100-31.jpeg brought their voice bade. How, but also don't\",\n",
       " '1-100-32.jpeg - --',\n",
       " '1-100-33.jpeg ',\n",
       " '1-100-34.jpeg am',\n",
       " '1-100-4.jpeg to take me to open to a new social',\n",
       " '1-100-5.jpeg to take me to open up to a new social',\n",
       " \"1-100-6.jpeg group, that's certainly how I behave, I'm\",\n",
       " \"1-100-7.jpeg realizing. So stop doing that! I'm\",\n",
       " '1-100-8.jpeg ',\n",
       " '1-100-9.jpeg 10:47pm Andrew showed up unexpectedly while I was',\n",
       " '1-101-0.jpeg 4/23/23 do that. As I left I heard some student',\n",
       " '1-101-1.jpeg cont. playing sex outside the dorms. I stopped and',\n",
       " '1-101-10.jpeg ',\n",
       " '1-101-11.jpeg 12:25am Okay looking a. big band (seems']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\pithy\\\\Documents\\\\journalocr'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
